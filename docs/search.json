[
  {
    "objectID": "LLVMs.html",
    "href": "LLVMs.html",
    "title": "LLVMs",
    "section": "",
    "text": "Diffusion Models\n\n\n\n\n\n\n\npaper\n\n\nDiffusion\n\n\n\n\n\n\n\n\n\n\n\nOct 12, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Raviteja Chukkapalli",
    "section": "",
    "text": "ABOUT ME\nMaster’s student in computer science at NYU Courant. I hate Dota 2. Making LLMs play games. Checkout Random.\n\nPreviously:\n\nComputer vision engineer, Vimaan, Bangalore.\nEngineer, Tonbo Imaging, Bangalore.\nB.E. BITS Pilani, Hyderabad.\n\n\n\nPreviously Worked on:\n\nComputer Vision\n\nHuman Object interaction\nObject detection\nRe-Identification\nMulti-camera object tracking\n\nVision + language\n\nMulti-Object Scene Generation (Diffusion Models)\nImage retrieval\nHuman Object Interaction\nOCR\n\nEngineering\n\nModel optimization\nDeployment on edge devices\nDataset collection, annotation."
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Notes",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Graph Relative Positional Encoding\n\n\n\n\n\n\n\npaper\n\n\nicml\n\n\ntransformer\n\n\ndrug discovery\n\n\ngraph\n\n\n\n\n\n\n\n\n\n\n\nFeb 1, 2023\n\n\n\n\n\n\n  \n\n\n\n\nEnd to End Object Detection with Transformers(DeTr)\n\n\n\n\n\n\n\nCode Walkthrough\n\n\n\n\n\n\n\n\n\n\n\nJan 5, 2023\n\n\n\n\n\n\n  \n\n\n\n\nPlan a trip!\n\n\n\n\n\n\n\nclassical AI\n\n\nInteractive\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/detr.html",
    "href": "posts/detr.html",
    "title": "End to End Object Detection with Transformers(DeTr)",
    "section": "",
    "text": "The evolution CNNs from classifiers to object detectors (CNN -> RCNN -> Fast-RCNN -> Faster-RCNN ….) is very interesting and rightly has a lot of articles written on it. If you are looking to read on this topic, I suggest you check this series post1, post2, post3, post4.\nThe transformer is another architecture that has a similar story. The adaptation of transformers from NLP to vision and vision + language tasks is very interesting. Today transformer-based architectures can solve various vision tasks like object detection(Carion et al. 2020), Human-Object Interaction Detection(Tamura, Ohashi, and Yoshinaga 2021), tracking(Zeng et al. 2022) etc. In this post, I go over one such adaptation of the transformer for object detection(DeTr) from (Carion et al. 2020), code open sourced.\nThere is a lot of content covering DeTr(yannic, HuggingFace). In this post, I go over a few parts of the implementation provided by the authors, that I believe are essential to understanding and adapting DeTr for different use cases. I assume the reader is familiar with object detection architectures and Transformers in general."
  },
  {
    "objectID": "posts/detr.html#forward-pass",
    "href": "posts/detr.html#forward-pass",
    "title": "End to End Object Detection with Transformers(DeTr)",
    "section": "Forward Pass",
    "text": "Forward Pass\n\nWe can start inference by calling the model with the image tensor.\nop = model(img[None, :])\nHere img is the image tensor of shape (C, H, W). we index it with [None, :] to add an extra dimension that signifies batch size(here it is 1). So the final shape of the input tensor is (BS, C, H, W).\nThe above call results in a call to the forward method of the model object(DeTr) which is shown below.\n\n\n\n\ndetr/models/detr.py: 44\n\n1.  def forward(self, samples: NestedTensor):\n2.     if isinstance(samples, (list, torch.Tensor)):\n3.         samples = nested_tensor_from_tensor_list(samples)\n4.     features, pos = self.backbone(samples)\n5.\n6.     src, mask = features[-1].decompose()\n7.     assert mask is not None\n8.     hs = self.transformer(self.input_proj(src), mask, self.query_embed.weight, pos[-1])[0]\n9.\n10.    outputs_class = self.class_embed(hs)\n11.    outputs_coord = self.bbox_embed(hs).sigmoid()\n12.    out = {'pred_logits': outputs_class[-1], 'pred_boxes': outputs_coord[-1]}\n13.    if self.aux_loss:\n14.        out['aux_outputs'] = self._set_aux_loss(outputs_class, outputs_coord)\n15.    return out\n\n\n\nI’ll go over the above function line by line and explain it.\n\n\n\nLines 2-3\n\nif isinstance(samples, (list, torch.Tensor)):\n    samples = nested_tensor_from_tensor_list(samples)\n\n\nThe modules in this repo are written to expect input of type NestedTensor, but if we send in 4d tensor (BS, C, H, W) or a list of 3d tensors (C, H, W) they are first converted to NestTensors using nested_tensor_from_tensor_list().\nIt looks like this.\n\n\n\n\ndetr/util/misc.py: 307\n\n1.  def nested_tensor_from_tensor_list(tensor_list: List[Tensor]):\n2.      if tensor_list[0].ndim == 3:\n3.          if torchvision._is_tracing():\n4.              # nested_tensor_from_tensor_list() does not export well to ONNX\n5.              # call _onnx_nested_tensor_from_tensor_list() instead\n6.              return _onnx_nested_tensor_from_tensor_list(tensor_list)\n7.  \n8.          # TODO make it support different-sized images\n9.          max_size = _max_by_axis([list(img.shape) for img in tensor_list])\n10.         # min_size = tuple(min(s) for s in zip(*[img.shape for img in tensor_list]))\n11.         batch_shape = [len(tensor_list)] + max_size\n12.         b, c, h, w = batch_shape\n13.         dtype = tensor_list[0].dtype\n14.         device = tensor_list[0].device\n15.         tensor = torch.zeros(batch_shape, dtype=dtype, device=device)\n16.         mask = torch.ones((b, h, w), dtype=torch.bool, device=device)\n17.         for img, pad_img, m in zip(tensor_list, tensor, mask):\n18.             pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n19.             m[: img.shape[1], :img.shape[2]] = False\n20.     else:\n21.         raise ValueError('not supported')\n22.     return NestedTensor(tensor, mask)\n\n\n\nif tensor_list[0].ndim == 3 is a check to see if the images have 3 channels.\nif torchvision._is_tracing() is a check to see if tracing is running at the moment to export the model to ONNX format. exported ONNX model is used for inference or optimized further using Hardware specific SDK like TensoRT etc.\nWe need not go in-depth into this as our focus is on understanding DeTr.\nIn the rest of the function we calculate the maximum of channels, width, and height of all the images in the batch.\nThen create a padded image of size (max_channels, max_height, max_width) for each image and copy the image data to the padded image.\nSince now we have created a padded image, we keep track of what part of this padded image has actual image content by creating a binary mask.\nWhy do we need to do this?\nEven though the transformer can handle variable sequence lengths. we do this to make the length of all the sequences in a batch to be same.\nThis is needed to take advantage of computation parallelism in accelerators like GPU, TPU, to some extent even a CPU these days.\nSince sequence length input to the transformer is determined by the image(will show this later) so we make all the images in the batch the same size.\nThis is similar to padding in transformers. And the mask is similar to the padding mask in transformers, it is used to stop attention modules from attending to the pad embeddings.\nWe use the image data tensor and mask tensor to create NestedTensor object, which is used as a container to store them.\nNestedTensor\n\nA class representing objects containing two fields, tensors and masks to store the image tensors and masks we calculated above.\nsupports methods [to(), decompose()] and also implements dunder repr for printing.\nto(): as expected moves the fields of the object (tensors, fields) between the device and host processors i.e, GPU, and CPU respectively.\ndecompose(): returns the fields as a tuple. \n\n\n\n\nLine 4\n\nfeatures, pos = self.backbone(samples)\n\n\nNow that data preparation is completed we begin inference and the first setup is to use the backbone for feature extraction.\nSo we call the backbone with NestedTensor we created as the argument.\nThe backbone extracts features(BS, C, H, W) from the images. these features are then converted to a sequence of embeddings(BS, HW, C) and passed to the transformer to do object detection.\nTransformer handles the sequence as if it is a set, so it does not get the positional information that is required to perform object detection. to tackle this transformers in NLP use a positional embedding(usually super-imposed on word embeddings) that encodes the position of the word in a sentence. In DeTr the positional information is 2 dimensional i.e, Height and Width dimensions. DeTr does this by extending the 1d positional embedding from (Vaswani et al. 2017) to two dimensions.\nPositional embedding calculation is also part of the backbone in DeTr.\nBackbone is of type Joiner. Joiner is an nn.Sequential module that contains the actual backbone(CNN) and a positional_embedding module.\n\nnn.Sequential basically calls the nn.Modules in the order they are passed to it.\nSo we first infer CNN followed by positional_embedding module on CNN output.\n\n\n\nBackbone - CNN\n\nHow does CNN inference work in DeTr?\nWe can choose any of the resnet models as the backbone.\nChosen resnet model is loaded from torchvision.models with pre-trained weights and batchnorm set to torchvision.ops.FrozenBatchNorm2d.\nIn fine-tuning and inference BatchNorm is usually frozen, The logic behind freezing batchnorm for fine-tuning is that the batch statistics on fine-tuning dataset can be different from that of pre-trained dataset. And not freezing the batchnorm will cause the layer to learn the new statistics which means the rest of the layers now have to adapt to new statistics. so all the weights have to change significantly. Since the fine-tuning dataset is usually small compared to the pre-trained dataset trying to learn all the weights again will not result in a good model compared to freezing the batchnorm and nudging the weights a little with smaller learning rates. In practice, this should hold as long as both the fine-tuning and pre-trained datasets are reasonably similar.\nResnet models are classifiers, they have a CNN backbone that returns the feature map (BS, C, H, W), followed by pooling and MLPs for classification.\nWe only need the output of CNN, If the DeTr is performing object detection we collect the output of layer4 from the CNN.\nThat is if DeTr is performing instance segmentation we collect the output of layer1 to layer4 of resnet.\nThis output selection can be performed using torchvision.models._utils.IntermediateLayerGetter Which is a module wrapper that lets us collect intermediate layer outputs. It makes some assumptions about the module it is wrapping. refer docstring here for details.\nUsage looks like IntermediateLayerGetter(backbone, return_layers={'layer4': \"0\"}) or IntermediateLayerGetter(backbone, return_layers={\"layer1\": \"0\", \"layer2\": \"1\", \"layer3\": \"2\", \"layer4\": \"3\"})\nCNN takes the image tensor of shape (BS, C, H, W) and returns a feature map of shape (BS, C1, H1, W1) where C1 > C, H1 < H, W1 < W.\nThe mask using which we are keeping track of the location of image content in the padded image tensor also needs to be resized to indicate the region in the feature map that is related to the image content of the padded image tensor.\nThis is done by creating a new nn.Module Backbone and using it to do the resize of mask operation after CNN inference. The implementation is linked below.\n\n\n\n\ndetr/models/backbone.py: 72\n\n1. def forward(self, tensor_list: NestedTensor):\n2.     xs = self.body(tensor_list.tensors)  # CNN inference\n3.     out: Dict[str, NestedTensor] = {}\n4.     for name, x in xs.items():\n5.         m = tensor_list.mask\n6.         assert m is not None\n7.         mask = F.interpolate(m[None].float(), size=x.shape[-2:]).to(torch.bool)[0]  # Mask Resize\n8.         out[name] = NestedTensor(x, mask)\n9.     return out\n\n\n\nThe resize is done using torch.nn.functional.interpolate.\n\n\n\nBackbone - Positional Embedding\n\nThe next step is to calculate the positional embedding to capture the 2d positional information of the feature map. This is done as shown below.\n\n\n\n\ndetr/models/position_encoding.py: 28\n\n1.  def forward(self, tensor_list: NestedTensor):\n2.      x = tensor_list.tensors\n3.      mask = tensor_list.mask\n4.      assert mask is not None\n5.      not_mask = ~mask\n6.      y_embed = not_mask.cumsum(1, dtype=torch.float32)\n7.      x_embed = not_mask.cumsum(2, dtype=torch.float32)\n8.      if self.normalize:\n9.          eps = 1e-6\n10.         y_embed = y_embed / (y_embed[:, -1:, :] + eps) * self.scale\n11.         x_embed = x_embed / (x_embed[:, :, -1:] + eps) * self.scale\n12.  \n13.     dim_t = torch.arange(self.num_pos_feats, dtype=torch.float32, device=x.device)\n14.     dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n15. \n16.     pos_x = x_embed[:, :, :, None] / dim_t\n17.     pos_y = y_embed[:, :, :, None] / dim_t\n18.     pos_x = torch.stack((pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4).flatten(3)\n19.     pos_y = torch.stack((pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4).flatten(3)\n20.     pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)\n21.     return pos\n\n\n\n(Vaswani et al. 2017) experiments with learned positional embeddings and sinusoidal embeddings. we will discuss 2d sinusoidal embedding. Learned 2d embedding is straightforward it is learnt, and involves no specific engineering or tricks.\nHeight, Width sinusoidal positional vectors are half the size of transformer embeddings. so that both concatenated(pos embedding) would be the size of the transformer embeddings.\nposition embedding is then superimposed on input embeddings of the transformer encoder.\nI’ll show the results of the above function on a feature map of width 24 and height 16.\nThe implementation uses x, and y terminology instead of width and height. here x is along the width and y is along the height dimension.\nIn lines 5-7 we calculate x_embed and y_embed, these are mappings of each pixel location on the feature map to integers representing x, and y locations respectively.\nIn Lines 8-11 we normalize these locations to integer maps so that the largest width and height are represented by 2\\(\\pi\\). The below visualization shows the integer and normalized representations of height and width.\n\n\n\nI’ll use a 128-dimensional positional embedding. This implies that x and y positional embeddings are of 64 dimensions each.\n(Vaswani et al. 2017) defines sinusoidal embeddings as follows.\n\n\\(\\Large PE_{(pos,2i)} = sin(pos/10000^{\\frac{2i}{d_{model}}})\\)\n\\(\\Large PE_{(pos,2i+1)} = cos(pos/10000^{\\frac{2i}{d_{model}}})\\)\n\nAs the dimension of the embedding increases the wavelength of sin and cos increases. the last dimension satisfies \\(2i = d_{model}\\) so the wavelength becomes 10000 times larger i.e, the last dimension changes very little between consecutive positions.\nLines 13-14 calculate this wavelength. The below visualization shows the wavelengths for each embedding dimension.\n\n\n\nSinusoidal embeddings has the property that any \\(PE_{pos+k}\\) can be represented as a linear function of \\(PE_{pos}\\) this lets the transformer attend by relative positions.\nFinally in lines 16-19 we calculate the x, and y positional embeddings. The below visualization shows these embeddings.\n\n\n\n\n\nIn line 20 we concatenate x, and y positional embeddings of 64 dimensions each to create the 2d positional embedding of 128 dimensions.\nThis concludes CNN feature extraction and positional embedding calculation."
  },
  {
    "objectID": "posts/detr.html#line-8",
    "href": "posts/detr.html#line-8",
    "title": "End to End Object Detection with Transformers(DeTr)",
    "section": "Line 8",
    "text": "Line 8\n\nhs = self.transformer(self.input_proj(src), mask, self.query_embed.weight, pos[-1])[0]\n\n\nWe further reduce the number of channels in the feature map output from CNN to match the embedding size of the transformer. This is done by self.input_proj Which is a single nn.Conv2d layer with a convolutional kernel size of (1,1). The feature map output from input_proj of shape (BS, C, H, W) is passed to the transformer.\nTransformer parses feature map into a sequence of embeddings of shape (BS, HW, C) which can be interpreted as a BS-sized batch of HW number of embeddings, each of size C.\nMask we calculated of shape (BS, H, W) is interpreted as a mask of shape (BS, HW). This is similar to the padding mask in NLP indicating pad tokens. here mask indicates the padding added to the image.\nPositional embedding of shape (BS, C, H, W) is passed to the transformer. This is also interpreted as (BS, HW, C) positional embedding sequence and superimposed on input embeddings.\nThis concluded all the input required for the transformer encoder, But the transformer decoder needs query input, DeTr learns these query embeddings and calls them object query embeddings. The number of object query embeddings is fixed during training and this determines the maximum number of objects the architecture can predict.\nThese object queries are typically very small compared to the number of anchors used in FRCNN-like architectures.\nTransformer decoder returns one result embedding for each query embedding then we use MLPs to calculate the bounding box location and object class from the result embedding.\nSo the query embeddings effectively act as a replacement for anchor boxes and DeTr predicts a set of objects available in the image."
  },
  {
    "objectID": "posts/grpe.html",
    "href": "posts/grpe.html",
    "title": "Graph Relative Positional Encoding",
    "section": "",
    "text": "Introduction\nA Graph is a versatile data structure that can be thought of as representing relations(edges) between a set of objects(nodes). A transformer is an architecture that operates on sets. So can we use transformers to learn representations of graphs? Today we cover a paper from ICLR 2022 that does exactly this.\n\nGRPE: Relative Positional Encoding for Graph Transformer\nImplementation\n\nThis paper and our post deals with molecular graphs as part of drug discovery pipelines. Here the task is to predict various properties of molecules like shape, reactivity etc. from their graph representations.\n\n\nHow To Process Graphs With Transformers?\nTo answer this, let’s review what we already know, how do transformers process text and images? Neither of them is a set. What do we mean when we say the transformer treats input data as a set?\n\n\nSet Processing\n\nWe present a simplified view of basic transformer encoder operation(self-attention mechanism) to explain the set processing aspect of the transformer. The actual operation involves extracting Query, Key, and Value(Q, K, V) information from input elements. Attention scores \\(A_i\\) and output \\(z_i\\) are calculated using Q, K, and V.\nWhen a transformer receives 10 input elements, it calculates for each element \\(E_{i}\\).\n\nAttention scores \\([A_{i1}, A_{i2},...A_{ii},.... A_{i10}]\\) w.r.t all elements including itself, attention \\(A_{i1}\\) signifies the amount of attention the model has to pay to the first element.\nThen the elements are combined \\[\\Large Z_i = \\hat{A}_{i1} \\cdot E_{1} + ..... + \\hat{A}_{i10} \\cdot E_{10} \\]\n\\([\\hat{A}_{1}.... \\hat{A}_{n}]\\) signifies softmax operation on \\([{A}_{1}.... {A}_{n}]\\). This operation is performed multiple times in a transformer along with feed-forward and non-linearity operations on individual elements.\n\nNow, if you want to create a classifier out of this architecture,\n\none can pass an extra dummy input element, let’s call it \\(E_{cls}\\), then we can pass the output \\(z_{cls}\\) through an MLP and use it as a classifier.\nOr, we can collect \\([z_{1}, z_{2},...... z_{10}]\\) and average them and pass the result through an MLP and use it as a classifier.\n\nIf you look at the classifier we have designed above, altering the position of input elements from \\([E_{1}, E_{2},.... E_{10}]\\) to \\([E_{10}, E_{9},.... E_{1}]\\) makes no difference to the output(Think in terms of the operations performed on the input).\nThe architecture essentially treats the input elements as if they contain all the information in their representation and not in any other aspect like their absolute or relative position in the input sequence.\nThis is why we say the transformer treats the input as a set.\n\n\n\ntext Processing\n\nText is a sequence, but not a set. Elements in a sequence have unique positional information associated with them. The position of a word carries useful information. e.g. “I am tall” and “am I tall” do not have the same meaning.\nWe want our model to use this positional information, as it is important for the meaning of a sentence.\nW.K.T transformers treat the input sequence as a set. But transformers are quite famous in NLP, how are they processing text as sets?\nA Transformer believes all the information about each input element is present in the representation of the element. So explicitly add position information to the word representations. e.g. “I-1 am-2 tall-3”. This is a simplification. The exact way we do it is by superimposing position information on word representation. That is embedding[\"tall\"] = embedding[\"tall\"] + embedding[\"3\"].\nYou can look at this post to further understand how the position information is added to the input representation.\nNow that we added position information to word representation, the transformer can treat the input like a set and still not lose the position information.\n\n\n\nImage Processing\n\nImage is a 2 dimensional grid(grayscale), the two dimensions are along height and width.\nFor an image of size say 100x100, we may split the image into 100 patches each of size 10x10 and convert the patches into embeddings(depth wise Convolution/linearize and dot product) and superimpose 2d positional information onto these embeddings and pass the sequence to the transformer.\n\n\n\nGraph Processing\n\nIn the text and image we saw that the funda was to destroy the structure of your input data and format it into a sequence of elements. Then identify the structural information that is essential for the task and add it to the representation of individual elements.\nWe can ignore the graph structure and create a sequence by randomly ordering nodes in the graph. This gives us the sequence we needed.\nthere is no notion of the ordering of nodes in a graph, so we can not add structural information to node embeddings based on the order.\nGraph nodes do not have absolute positional information, any node can be the first node. they have relative positional information e.g. nodes at 1, 2, 3… hops away from a node.\nWhen adding absolute positional information every element in the sequence has unique positional information added to it. But in relative positioning, the position of a node varies based on the node relative to which the measurement is taken. So if a graph has n nodes, each node has n relative position values 1 w.r.t each node in the graph.\nSince each element in the sequence has multiple relative position values it is not possible to add them all to the representation of the element. (Shaw, Uszkoreit, and Vaswani 2018) suggests the following modifications to the attention mechanism to incorporate the relative position information. \\[\\Large A_{ij} = \\frac{x_{i}W^Q(x_jW^K+a_{ij}^K)^T}{\\sqrt{d_z}}\\] \\[\\Large z_i = \\hat{A_{ij}}(x_jW^V+a_{ij}^V)\\]\n\\(\\Large a_{ij}^K\\) and \\(\\Large a_{ij}^V\\) are learnt relative positional embeddings. key, value of components of \\(\\Large a_{ij}\\) are learnt seperately to simplify computation.\nRecent work from (Park et al. 2022) shows that considering node-spatial, node-edge interactions when adding structural information improves the performance on drug property prediction tasks. They modify the attention mechanism as follows. \\[\\Large b_{ij}^{spatial} = q_i\\mathcal{P}_{\\psi(i,j)}^{query}+k_j\\mathcal{P}_{\\psi(i,j)}^{key}\\] \\[\\Large b_{ij}^{edge} = q_i\\mathcal{E}_{e_{ij}}^{query}+k_j\\mathcal{E}_{e_{ij}}^{key}\\] \\[\\Large A_{ij} = \\frac{q_i \\cdot k_j + b_{ij}^{spatial} + b_{ij}^{edge}}{\\sqrt{d_z}}\\] \\[\\Large z_i = \\sum_{j=1}^{N} \\hat{A}_{ij}(v_j+\\mathcal{P}_{\\psi(i,j)}^{value}+\\mathcal{E}_{e_{ij}}^{value})\\]\nWhere \\(\\Large \\psi(i,j)\\) is the shortest path distance between the nodes i, j. \\(\\Large e_{ij}\\) is the type of edge between node i, j(e.g: double bond, single bond etc). \\(\\Large \\mathcal{P}\\) is the spatial encoding(distance), \\(\\Large \\mathcal{E}\\) is the edge encoding(edge type). \\(\\Large b_{ij}^{spatial}\\) captures spatial relation between two nodes while considering the node-spatial interaction. \\(\\Large b_{ij}^{edge}\\) captures the edge between node i and j while considering node-edge interaction, the edge type was not considered by (Shaw, Uszkoreit, and Vaswani 2018). Both \\(\\Large \\mathcal{P}\\) and \\(\\Large \\mathcal{E}\\) are learnt during training.\nAttention modification visualized. \n(Park et al. 2022) improves performance on various datasets.   \n\n\n\n\n\n\nReferences\n\nPark, Wonpyo, Woong-Gi Chang, Donggeon Lee, Juntae Kim, et al. 2022. “Grpe: Relative Positional Encoding for Graph Transformer.” In ICLR2022 Machine Learning for Drug Discovery.\n\n\nShaw, Peter, Jakob Uszkoreit, and Ashish Vaswani. 2018. “Self-Attention with Relative Position Representations.” arXiv Preprint arXiv:1803.02155."
  },
  {
    "objectID": "posts/mdp.html",
    "href": "posts/mdp.html",
    "title": "Plan a trip!",
    "section": "",
    "text": "Rules\n\nS is the starting location\nBlue suqares represent a river. Which you can cross using a bridge with some probability of getting stuck in the traffic and not reaching D.\nD is the destination.\nUse input boxes on the right to set a reward for reaching the location.\nThe arrows on the left map represent the optimal policy for a traveller going from S to D and collecting as much reward as possible on the way."
  },
  {
    "objectID": "posts_LLVMs/diffusion.html#sampling-from-distributions",
    "href": "posts_LLVMs/diffusion.html#sampling-from-distributions",
    "title": "Diffusion Models",
    "section": "Sampling from Distributions",
    "text": "Sampling from Distributions\n\nUsing Random Noise"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Random",
    "section": "",
    "text": "LLMs playing games\n\n\n\n\n\n\n\nRandom\n\n\nVisualization\n\n\n\n\n\n\n\n\n\n\n\nMar 15, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/llm_agents.html",
    "href": "projects/llm_agents.html",
    "title": "LLMs playing games",
    "section": "",
    "text": "Can LLMs plan and reason?\nThis question is asked a lot these days! and not so coincidentally a lot of research groups are working on this. One direction this research has taken is to use games to evaluate this hypothesis.\nThe first step would be to make an LLM understand the game state (observations) and take valid actions(play moves). Knowing the strong instruction-following capacity of LLMs (instruction-tuned LLM) this should be achievable!\nHere are some visualizations of LLMs playing against classical AI agents."
  },
  {
    "objectID": "projects/llm_agents.html#tic-tac-toe-propmt-based-agents",
    "href": "projects/llm_agents.html#tic-tac-toe-propmt-based-agents",
    "title": "LLMs playing games",
    "section": "Tic-Tac-Toe, Propmt-based Agents",
    "text": "Tic-Tac-Toe, Propmt-based Agents\n\nGame: Tic-Tac-Toe\nLLMs: LLAMA2 7B, MISTRAL 7B\nAgent Design: Propmt based, i.e, game instructions and current state are given in the input prompt to the LLM.\nReasoning Module: None/LLM itself.\n\nThe Player using 🟥 Is the LLM Agent\nThe Player using 🟦 Is Monte Carlo Tree Search.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: LLAMA2 7B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: MISTRAL 7B"
  },
  {
    "objectID": "projects/llm_agents.html#tic-tac-toe-chain-of-though-prompt-based-agents",
    "href": "projects/llm_agents.html#tic-tac-toe-chain-of-though-prompt-based-agents",
    "title": "LLMs playing games",
    "section": "Tic-Tac-Toe, Chain-of-Though Prompt based Agents",
    "text": "Tic-Tac-Toe, Chain-of-Though Prompt based Agents\n\nllama2_7b\n\nTopmost paragraph is the system prompt.\nSecond paragraph is the user prompt.\nLLM response is the last paragraph.\n\n\nVideo"
  }
]