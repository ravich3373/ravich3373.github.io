<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2024-05-02">

<title>Raviteja Chukkapalli - LLMs playing games</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-6C8LEPK16T"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-6C8LEPK16T', { 'anonymize_ip': true});
</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Raviteja Chukkapalli</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html">
 <span class="menu-text">Home</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../notes.html">
 <span class="menu-text">Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../posts.html">
 <span class="menu-text">Posts</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../projects.html">
 <span class="menu-text">Projects</span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#abstracttldr" id="toc-abstracttldr" class="nav-link active" data-scroll-target="#abstracttldr">Abstract/TLDR</a></li>
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#evaluation" id="toc-evaluation" class="nav-link" data-scroll-target="#evaluation">Evaluation</a></li>
  <li><a href="#architectural-choices-involved-in-llm-based-agent-design" id="toc-architectural-choices-involved-in-llm-based-agent-design" class="nav-link" data-scroll-target="#architectural-choices-involved-in-llm-based-agent-design">Architectural choices involved in LLM Based Agent design</a></li>
  <li><a href="#in-context-methods" id="toc-in-context-methods" class="nav-link" data-scroll-target="#in-context-methods">In-Context Methods</a>
  <ul class="collapse">
  <li><a href="#our-approach" id="toc-our-approach" class="nav-link" data-scroll-target="#our-approach">Our-Approach</a></li>
  </ul></li>
  <li><a href="#in-weight-methods" id="toc-in-weight-methods" class="nav-link" data-scroll-target="#in-weight-methods">In-Weight Methods</a>
  <ul class="collapse">
  <li><a href="#our-approach-1" id="toc-our-approach-1" class="nav-link" data-scroll-target="#our-approach-1">Our-Approach</a></li>
  </ul></li>
  <li><a href="#appendix" id="toc-appendix" class="nav-link" data-scroll-target="#appendix">Appendix</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">LLMs playing games</h1>
  <div class="quarto-categories">
    <div class="quarto-category">Random</div>
    <div class="quarto-category">Visualization</div>
  </div>
  </div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">May 2, 2024</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<p><strong>Can LLMs plan and reason?</strong></p>
<div class="callout-note callout callout-style-simple">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>This page is alive i.e, I‚Äôll be adding new visualizations (new <code>LLM</code>, <code>Game</code>, <code>Planning approach</code> etc.).</p>
</div>
</div>
</div>
<section id="abstracttldr" class="level2">
<h2 class="anchored" data-anchor-id="abstracttldr">Abstract/TLDR</h2>
<p>We explored the design space of LLM agents to improve the agent planning capabilities and our initial results show that for In-context based approaches MCTS is a good prior to encode into the prompts and In-weight approaches still outperform in-context approaches.</p>
<p>We hope to continue with the experiments and refine the in-context MCTS further.</p>
<p>Code for In-context techniques <a href="https://github.com/ravich3373/GTBench">github</a> and In-weight experiments <a href="https://github.com/lakshayt7/LLM-RLHF-DQN">github</a>.</p>
</section>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>In-context learning and instruction following capabilities of LLMs combined with their large world knowledge make them great at solving system 1<span class="citation" data-cites="kahneman2011thinking">(<a href="#ref-kahneman2011thinking" role="doc-biblioref">Kahneman 2011</a>)</span> tasks. The next obvious question would be to evaluate their performance in tasks that require deliberate thinking and planning i.e.&nbsp;system 2<span class="citation" data-cites="kahneman2011thinking">(<a href="#ref-kahneman2011thinking" role="doc-biblioref">Kahneman 2011</a>)</span> tasks.</p>
</section>
<section id="evaluation" class="level2">
<h2 class="anchored" data-anchor-id="evaluation">Evaluation</h2>
<p>Recent works try to evaluate system 2 capabilites of LLMs through</p>
<ol type="1">
<li>Planning to solve tasks in interactive text-world environments
<ol type="1">
<li>ALFWorld <span class="citation" data-cites="shridhar2020alfworld">(<a href="#ref-shridhar2020alfworld" role="doc-biblioref">Shridhar et al. 2020</a>)</span>.</li>
<li>Web-Shop <span class="citation" data-cites="yao2022webshop">(<a href="#ref-yao2022webshop" role="doc-biblioref">Yao et al. 2022</a>)</span>.</li>
</ol></li>
<li>Fact Checking tasks
<ol type="1">
<li>FEVER <span class="citation" data-cites="thorne2018fever">(<a href="#ref-thorne2018fever" role="doc-biblioref">Thorne et al. 2018</a>)</span></li>
</ol></li>
<li>Question Answering tasks
<ol type="1">
<li>HotPotQA <span class="citation" data-cites="yang2018hotpotqa">(<a href="#ref-yang2018hotpotqa" role="doc-biblioref">Yang et al. 2018</a>)</span></li>
</ol></li>
</ol>
<p>The issue with these evaluations is that an LLM with prior knowledge of facts can easily solve the tasks without deliberate thinking and planning.</p>
<p>In their new work <span class="citation" data-cites="duan2024gtbench">(<a href="#ref-duan2024gtbench" role="doc-biblioref">Duan et al. 2024</a>)</span> Propose to evaluate deliberate thinking through game-theoretic tasks.</p>
<blockquote class="blockquote">
<p>‚Äúthe extensive background and intricate details involved in role-play-based games dilute the pureness of logic and strategic reasoning that is typically found in game theoretic tasks‚Äù</p>
</blockquote>
<p>They Propose GTBench a collection of 11 games and suggest evaluating LLM capabilities through LLM vs LLM or LLM vs Classical Agent games.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="agents/gtbench_games.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">11 Games part of GTBench <span class="citation" data-cites="duan2024gtbench">(<a href="#ref-duan2024gtbench" role="doc-biblioref">Duan et al. 2024</a>)</span></figcaption><p></p>
</figure>
</div>
<p>We use GTBench to evaluate the Planning capabilities of LLMs.</p>
</section>
<section id="architectural-choices-involved-in-llm-based-agent-design" class="level2">
<h2 class="anchored" data-anchor-id="architectural-choices-involved-in-llm-based-agent-design">Architectural choices involved in LLM Based Agent design</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="agents/llm_arch.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">As Surveyed by <span class="citation" data-cites="huang2024understanding">(<a href="#ref-huang2024understanding" role="doc-biblioref">Huang et al. 2024</a>)</span></figcaption><p></p>
</figure>
</div>
<section id="task-decomposition" class="level4">
<h4 class="anchored" data-anchor-id="task-decomposition">Task Decomposition</h4>
<p>Almost all of the approaches use this technique. As the name says the prompts are designed to prompt the LLM to decompose the given task into subtasks and then tackle them.</p>
<p>ReAct, a prominent decomposition-based approach, provides in-context examples decomposing a couple of tasks, and expects the LLM would solve the problem similarly.</p>
</section>
<section id="multi-plan-selection" class="level4">
<h4 class="anchored" data-anchor-id="multi-plan-selection">Multi-Plan Selection</h4>
<p>Query the LLM multiple times to generate (unique) plans (use low temperature). and select one of the plans using the LLM itself or an external critique.</p>
</section>
<section id="external-planner-aided-planning" class="level4">
<h4 class="anchored" data-anchor-id="external-planner-aided-planning">External-Planner aided planning</h4>
<p>These approaches usually query the LLM with problem definition to generate problem PDDL (Planning Domain Definition Language) file assuming problem domain file is already present and use external planners like <a href="https://www.fast-downward.org/">Downward</a> to generate the plan.</p>
<p>Newer approaches also try generating domain files through LLM.</p>
</section>
<section id="reflection-and-refinement" class="level4">
<h4 class="anchored" data-anchor-id="reflection-and-refinement">Reflection and Refinement</h4>
<p>Use an external critic or the LLM itself to critic the generated plan and query the LLM to improve the plan according to the critique.</p>
</section>
<section id="memory-aided-planning" class="level4">
<h4 class="anchored" data-anchor-id="memory-aided-planning">Memory Aided Planning</h4>
<p>There are diverse approaches that use external memory, some use it to store experiences and then retrieve relevant experiences during Test time similar to RAG. Some try to do full Reinforcement Learning by storing states and action values in memory.</p>
</section>
</section>
<section id="in-context-methods" class="level2">
<h2 class="anchored" data-anchor-id="in-context-methods">In-Context Methods</h2>
<section id="our-approach" class="level3">
<h3 class="anchored" data-anchor-id="our-approach">Our-Approach</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="agents/prmpt_tech.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Progression of prompting techniques increasingly encoding planning prior into LLM <span class="citation" data-cites="yao2024tree">(<a href="#ref-yao2024tree" role="doc-biblioref">Yao et al. 2024</a>)</span></figcaption><p></p>
</figure>
</div>
<p>The progression of prompting techniques for planning, i.e.</p>
<p>Prompt -&gt; CoT (Chain of Thought)-&gt; ToT (Tree of Thought)</p>
<p>can be seen as an effort to encode the planning prior into the LLM prompts.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="agents/mcts.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Monte Carlo Tree Search <span class="citation" data-cites="swiechowski2023monte">(<a href="#ref-swiechowski2023monte" role="doc-biblioref">≈öwiechowski et al. 2023</a>)</span></figcaption><p></p>
</figure>
</div>
<p>A very strong planning technique that works well with deep learning is Monte Carlo Tree Search (MCTS).</p>
<p>We try to encode MCTS planning prior into the LLM prompts. We do this by designing prompts for</p>
<ol type="1">
<li>Selection</li>
<li>Expansion and Simulation</li>
<li>BackPropagation or Learning</li>
</ol>
<p>We also initialize an in-context knowledge that is empty at the start and LLM stores its learning into this memory during the course of the game. We call this In-context MCTS.</p>
<blockquote class="blockquote">
<p>In-Context MCTS</p>
</blockquote>
<p><span class="math display">\[
\begin{array}{rlr}
A_t &amp;= \text{Selection}(P_s, \theta, K_t, S_t) \\
A_{opp} &amp;= \text{Expansion}(P_e, \theta, K, S_t + A_t) \\
K_{t+1} &amp;= \text{BackProp}(P_t, \theta, K, S_t+A_t+A'_{opp}+A_{opp})
\end{array}
\]</span></p>
<section id="results" class="level4">
<h4 class="anchored" data-anchor-id="results">Results</h4>
<table class="table">
<colgroup>
<col style="width: 18%">
<col style="width: 23%">
<col style="width: 19%">
<col style="width: 19%">
<col style="width: 18%">
</colgroup>
<thead>
<tr class="header">
<th>NRA</th>
<th>Prompt-GPT4 Agent</th>
<th>CoT-GPT4 Agent</th>
<th>ToT-GPT4 Agent</th>
<th>IC-MCTS Agent</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>VS MCTS</td>
<td>-0.66</td>
<td>-0.3</td>
<td>-0.5</td>
<td>-0.75</td>
</tr>
<tr class="even">
<td>VS Rand-Agent</td>
<td>0.5</td>
<td>0.5454</td>
<td>0.25</td>
<td>0.3</td>
</tr>
</tbody>
</table>
</section>
<section id="progression-of-in-context-knowledge-during-a-game." class="level4">
<h4 class="anchored" data-anchor-id="progression-of-in-context-knowledge-during-a-game.">Progression of In-Context Knowledge during a game.</h4>
<ol type="1">
<li>At the Start of the Game</li>
</ol>
<blockquote class="blockquote">
<p>‚Äú‚Äù</p>
</blockquote>
<ol start="2" type="1">
<li>After Opponent Makes Fist Move.</li>
</ol>
<blockquote class="blockquote">
<p>The opponent‚Äôs move &lt;C2R2&gt; indicates a strategy of controlling the center of the grid, which is a common tactic in Tic Tac Toe as it provides the most opportunities for creating a winning line. This move also suggests that the opponent is willing to take risks, as they chose not to block my potential winning line. This knowledge, combined with the previously gained understanding of the opponent‚Äôs dynamic strategy and their focus on creating their own winning lines, suggests that the opponent is a risk-taking, offensive player. This knowledge can be used to anticipate the opponent‚Äôs moves and adjust my strategy accordingly, focusing on both blocking their potential winning lines and creating my own.</p>
</blockquote>
<ol start="3" type="1">
<li>After Losing the game to the Opponent.</li>
</ol>
<blockquote class="blockquote">
<p>The opponent‚Äôs move &lt;C2R3&gt; shows a strategic shift towards controlling the corners and edges of the grid. This move indicates that the opponent is not only focused on blocking my potential winning lines but also on creating their own. The opponent‚Äôs strategy seems to be dynamic, adapting based on the game‚Äôs progression. This knowledge, combined with the previously gained understanding, suggests that the opponent is a risk-taker who can change their strategy as needed. This knowledge can be used to anticipate the opponent‚Äôs moves and adjust my strategy accordingly. It is crucial to not only focus on blocking the opponent‚Äôs potential winning lines but also on creating my own. Additionally, it is important to consider the corners and edges of the grid as potential winning lines, as the opponent has shown a tendency to control these areas.</p>
</blockquote>
</section>
<section id="conclusion-in-context" class="level4">
<h4 class="anchored" data-anchor-id="conclusion-in-context">Conclusion (In-Context)</h4>
<ol type="1">
<li>Prompting performance often does not transfer across LLMs.</li>
<li>Knowledge in IC-MCTS becomes opponent specific. Memory needs to be persistent across multiple games and multiple opponents to learn a generic/useful strategy.</li>
<li>Computationally expensive at test time.</li>
<li>Current approaches are fragile and break easily.</li>
</ol>
</section>
</section>
</section>
<section id="in-weight-methods" class="level2">
<h2 class="anchored" data-anchor-id="in-weight-methods">In-Weight Methods</h2>
<section id="our-approach-1" class="level3">
<h3 class="anchored" data-anchor-id="our-approach-1">Our-Approach</h3>
<section id="rlhf-finetuning-for-strategic-reasoning" class="level4">
<h4 class="anchored" data-anchor-id="rlhf-finetuning-for-strategic-reasoning">RLHF finetuning for strategic reasoning</h4>
<p>A traditional RLHF pipeline has the following steps</p>
<ul>
<li>Gather preference data</li>
<li>Train a reward model</li>
<li>Use feedback from the reward model to train the LLM using PPO</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="agents/RLHF.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">A standard RLHF pipeline</figcaption><p></p>
</figure>
</div>
<p>We adapt this pipeline for training LLM agents to play strategic games. Instead of gathering preference data and then training a reward model, we directly train a Deep Q Network model using self play. This Deep Q Network model will then be used to provide reward values for responses from the LLM.</p>
<p>The whole pipeline is as shown below:</p>
<ul>
<li>Train DQN using self-play to play games from the Open Spiel framework</li>
<li>Evaluate the DQN against random agent and another DQN agent and store it in state buffer</li>
<li>Generate rewards for state, action pairs in the replay buffer</li>
<li>Generate prompts for each state, prompt LLM to get it‚Äôs response. Extract action using regex</li>
<li>Use reward from the DQN as feedback, train the LLM using PPO</li>
</ul>
<p>The above steps can be repeated for multiple games and a multi-game dataset of state/action pairs along with their rewards can be created. In particular we generate a five game dataset which includes data from the games Tic-Tac-Toe, Kuhn‚Äôs Poker, Nim, Liar‚Äôs dice, and Pig. The reason we restrict ourselves to these games is that they have an easy structure which makes it relatively simpler to engineer prompts and regexes for them.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="agents/DQN-RLHF.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">DQN RLHF pipeline for strategic reasoning over games</figcaption><p></p>
</figure>
</div>
</section>
<section id="results-1" class="level4">
<h4 class="anchored" data-anchor-id="results-1">Results</h4>
<table class="table">
<colgroup>
<col style="width: 31%">
<col style="width: 37%">
<col style="width: 31%">
</colgroup>
<thead>
<tr class="header">
<th>Normalized Relative Advantage</th>
<th>RLHF LLAMA2 (only ticatac toe data)</th>
<th>RLHF LLAMA2 (Multi-Game Data)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>VS LLAMA2</td>
<td>0.18</td>
<td>0.16</td>
</tr>
</tbody>
</table>
</section>
<section id="conclusion-in-weight-methods" class="level4">
<h4 class="anchored" data-anchor-id="conclusion-in-weight-methods">Conclusion (In-weight Methods)</h4>
<ol type="1">
<li>Limited generalization across games, however can help if games are similar</li>
<li>Expensive and time consuming need to finetune large number of parameters at train time</li>
<li>Lightweight at inference time</li>
<li>Not possible for closed source LLMs</li>
</ol>
</section>
</section>
</section>
<section id="appendix" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<section id="visualizations" class="level4">
<h4 class="anchored" data-anchor-id="visualizations">Visualizations</h4>
<p>Here are some visualizations of LLMs playing against classical AI agents.</p>
<ul>
<li>7B parameter LLMs are not great at the tasks as can be seen from the visualizations.</li>
<li>I‚Äôll soon upload the visualizations for larger LLMs.</li>
</ul>
</section>
<section id="tic-tac-toe-propmt-based-agents" class="level4">
<h4 class="anchored" data-anchor-id="tic-tac-toe-propmt-based-agents">Tic-Tac-Toe, Propmt-based Agents</h4>
<ul>
<li>Game: Tic-Tac-Toe</li>
<li>LLMs: LLAMA2 7B, MISTRAL 7B</li>
<li>Agent Design: Propmt based, i.e, game instructions and current state are given in the input prompt to the LLM.</li>
<li>Reasoning Module: None/LLM itself.</li>
</ul>
<p>The Player using üü• Is the LLM Agent</p>
<p>The Player using üü¶ Is Monte Carlo Tree Search.</p>
<div id="fig-elephants" class="quarto-layout-panel">
<figure class="figure">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="assets/llm_agents/llama2_7b/00000_fix.gif" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="assets/llm_agents/llama2_7b/00001_fix.gif" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="assets/llm_agents/llama2_7b/00002_fix.gif" class="img-fluid figure-img"></p>
</div>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;1: LLAMA2 7B</figcaption><p></p>
</figure>
</div>
<div id="fig-elephants2" class="quarto-layout-panel">
<figure class="figure">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="assets/llm_agents/mistral_7b/00000_fix.gif" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="assets/llm_agents/mistral_7b/00001_fix.gif" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="assets/llm_agents/mistral_7b/00002_fix.gif" class="img-fluid figure-img"></p>
</div>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;2: MISTRAL 7B</figcaption><p></p>
</figure>
</div>
</section>
<section id="tic-tac-toe-chain-of-though-prompt-based-agents" class="level4">
<h4 class="anchored" data-anchor-id="tic-tac-toe-chain-of-though-prompt-based-agents">Tic-Tac-Toe, Chain-of-Though Prompt based Agents</h4>
</section>
<section id="llama2_7b" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="llama2_7b">llama2_7b</h4>
<ol type="1">
<li>Topmost paragraph is the system prompt.</li>
<li>Second paragraph is the user prompt.</li>
<li>LLM response is the last paragraph.</li>
</ol>
<div class="column-page">
<p><video src="assets/llm_agents/cot_agents/llama2.mp4" class="img-fluid" controls=""><a href="assets/llm_agents/cot_agents/llama2.mp4">Video</a></video></p>
</div>



</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-duan2024gtbench" class="csl-entry" role="doc-biblioentry">
Duan, Jinhao, Renming Zhang, James Diffenderfer, Bhavya Kailkhura, Lichao Sun, Elias Stengel-Eskin, Mohit Bansal, Tianlong Chen, and Kaidi Xu. 2024. <span>‚ÄúGtbench: Uncovering the Strategic Reasoning Limitations of Llms via Game-Theoretic Evaluations.‚Äù</span> <em>arXiv Preprint arXiv:2402.12348</em>.
</div>
<div id="ref-huang2024understanding" class="csl-entry" role="doc-biblioentry">
Huang, Xu, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, and Enhong Chen. 2024. <span>‚ÄúUnderstanding the Planning of LLM Agents: A Survey.‚Äù</span> <em>arXiv Preprint arXiv:2402.02716</em>.
</div>
<div id="ref-kahneman2011thinking" class="csl-entry" role="doc-biblioentry">
Kahneman, Daniel. 2011. <em>Thinking, Fast and Slow</em>. macmillan.
</div>
<div id="ref-shridhar2020alfworld" class="csl-entry" role="doc-biblioentry">
Shridhar, Mohit, Xingdi Yuan, Marc-Alexandre C√¥t√©, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. 2020. <span>‚ÄúAlfworld: Aligning Text and Embodied Environments for Interactive Learning.‚Äù</span> <em>arXiv Preprint arXiv:2010.03768</em>.
</div>
<div id="ref-swiechowski2023monte" class="csl-entry" role="doc-biblioentry">
≈öwiechowski, Maciej, Konrad Godlewski, Bartosz Sawicki, and Jacek Ma≈Ñdziuk. 2023. <span>‚ÄúMonte Carlo Tree Search: A Review of Recent Modifications and Applications.‚Äù</span> <em>Artificial Intelligence Review</em> 56 (3): 2497‚Äì2562.
</div>
<div id="ref-thorne2018fever" class="csl-entry" role="doc-biblioentry">
Thorne, James, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. <span>‚ÄúFEVER: A Large-Scale Dataset for Fact Extraction and VERification.‚Äù</span> <em>arXiv Preprint arXiv:1803.05355</em>.
</div>
<div id="ref-yang2018hotpotqa" class="csl-entry" role="doc-biblioentry">
Yang, Zhilin, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. 2018. <span>‚ÄúHotpotQA: A Dataset for Diverse, Explainable Multi-Hop Question Answering.‚Äù</span> <em>arXiv Preprint arXiv:1809.09600</em>.
</div>
<div id="ref-yao2022webshop" class="csl-entry" role="doc-biblioentry">
Yao, Shunyu, Howard Chen, John Yang, and Karthik Narasimhan. 2022. <span>‚ÄúWebshop: Towards Scalable Real-World Web Interaction with Grounded Language Agents.‚Äù</span> <em>Advances in Neural Information Processing Systems</em> 35: 20744‚Äì57.
</div>
<div id="ref-yao2024tree" class="csl-entry" role="doc-biblioentry">
Yao, Shunyu, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2024. <span>‚ÄúTree of Thoughts: Deliberate Problem Solving with Large Language Models.‚Äù</span> <em>Advances in Neural Information Processing Systems</em> 36.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>