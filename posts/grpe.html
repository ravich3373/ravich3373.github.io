<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2023-02-01">

<title>Raviteja Chukkapalli - Graph Relative Positional Encoding</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-6C8LEPK16T"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-6C8LEPK16T', { 'anonymize_ip': true});
</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Raviteja Chukkapalli</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html">
 <span class="menu-text">Home</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../notes.html">
 <span class="menu-text">Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../posts.html">
 <span class="menu-text">Posts</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../projects.html">
 <span class="menu-text">Random</span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#how-to-process-graphs-with-transformers" id="toc-how-to-process-graphs-with-transformers" class="nav-link" data-scroll-target="#how-to-process-graphs-with-transformers">How To Process Graphs With Transformers?</a></li>
  <li><a href="#set-processing" id="toc-set-processing" class="nav-link" data-scroll-target="#set-processing">Set Processing</a></li>
  <li><a href="#text-processing" id="toc-text-processing" class="nav-link" data-scroll-target="#text-processing">text Processing</a></li>
  <li><a href="#image-processing" id="toc-image-processing" class="nav-link" data-scroll-target="#image-processing">Image Processing</a></li>
  <li><a href="#graph-processing" id="toc-graph-processing" class="nav-link" data-scroll-target="#graph-processing">Graph Processing</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Graph Relative Positional Encoding</h1>
  <div class="quarto-categories">
    <div class="quarto-category">paper</div>
    <div class="quarto-category">icml</div>
    <div class="quarto-category">transformer</div>
    <div class="quarto-category">drug discovery</div>
    <div class="quarto-category">graph</div>
  </div>
  </div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">February 1, 2023</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>A Graph is a versatile data structure that can be thought of as representing relations(edges) between a <strong>set</strong> of objects(nodes). A transformer is an architecture that operates on <strong>sets</strong>. So can we use transformers to learn representations of graphs? Today we cover a paper from ICLR 2022 that does exactly this.</p>
<ul>
<li><a href="https://iclr.cc/virtual/2022/workshop/4563#collapse8612">GRPE: Relative Positional Encoding for Graph Transformer</a></li>
<li><a href="https://github.com/lenscloth/GRPE">Implementation</a></li>
</ul>
<p>This paper and our post deals with molecular graphs as part of drug discovery pipelines. Here the task is to predict various properties of molecules like shape, reactivity etc. from their graph representations.</p>
</section>
<section id="how-to-process-graphs-with-transformers" class="level1">
<h1>How To Process Graphs With Transformers?</h1>
<p>To answer this, let’s review what we already know, how do transformers process text and images? Neither of them is a set. What do we mean when we say the transformer treats input data as a set?</p>
</section>
<section id="set-processing" class="level1">
<h1>Set Processing</h1>
<ol type="1">
<li>We present a <strong>simplified view</strong> of basic transformer encoder operation(self-attention mechanism) to explain the set processing aspect of the transformer. The actual operation involves extracting Query, Key, and Value(Q, K, V) information from input elements. Attention scores <span class="math inline">\(A_i\)</span> and output <span class="math inline">\(z_i\)</span> are calculated using Q, K, and V.</li>
<li>When a transformer receives 10 input elements, it calculates for each element <span class="math inline">\(E_{i}\)</span>.
<ol type="1">
<li>Attention scores <span class="math inline">\([A_{i1}, A_{i2},...A_{ii},.... A_{i10}]\)</span> w.r.t all elements including itself, attention <span class="math inline">\(A_{i1}\)</span> signifies the amount of attention the model has to pay to the first element.</li>
<li>Then the elements are combined <span class="math display">\[\Large Z_i = \hat{A}_{i1} \cdot E_{1} + ..... + \hat{A}_{i10} \cdot E_{10} \]</span></li>
<li><span class="math inline">\([\hat{A}_{1}.... \hat{A}_{n}]\)</span> signifies softmax operation on <span class="math inline">\([{A}_{1}.... {A}_{n}]\)</span>. This operation is performed multiple times in a transformer along with feed-forward and non-linearity operations on individual elements.</li>
</ol></li>
<li>Now, if you want to create a classifier out of this architecture,
<ol type="1">
<li>one can pass an extra dummy input element, let’s call it <span class="math inline">\(E_{cls}\)</span>, then we can pass the output <span class="math inline">\(z_{cls}\)</span> through an MLP and use it as a classifier.</li>
<li>Or, we can collect <span class="math inline">\([z_{1}, z_{2},...... z_{10}]\)</span> and average them and pass the result through an MLP and use it as a classifier.</li>
</ol></li>
<li>If you look at the classifier we have designed above, altering the position of input elements from <span class="math inline">\([E_{1}, E_{2},.... E_{10}]\)</span> to <span class="math inline">\([E_{10}, E_{9},.... E_{1}]\)</span> makes no difference to the output(Think in terms of the operations performed on the input).</li>
<li>The architecture essentially treats the input elements as if they contain all the information in their representation and not in any other aspect like their absolute or relative position in the input sequence.</li>
<li>This is why we say the transformer treats the input as a set.</li>
</ol>
</section>
<section id="text-processing" class="level1">
<h1>text Processing</h1>
<ol type="1">
<li>Text is a sequence, but not a set. Elements in a sequence have unique positional information associated with them. The position of a word carries useful information. e.g.&nbsp;“I am tall” and “am I tall” do not have the same meaning.</li>
<li>We want our model to use this positional information, as it is important for the meaning of a sentence.</li>
<li>W.K.T transformers treat the input sequence as a set. But transformers are quite famous in NLP, how are they processing text as sets?</li>
<li>A Transformer believes all the information about each input element is present in the representation of the element. So explicitly add position information to the word representations. e.g.&nbsp;“I-1 am-2 tall-3”. This is a simplification. The exact way we do it is by superimposing position information on word representation. That is <code>embedding["tall"] = embedding["tall"] + embedding["3"]</code>.</li>
<li>You can look at <a href="https://ravich3373.github.io/posts/detr.html">this post</a> to further understand how the position information is added to the input representation.</li>
<li>Now that we added position information to word representation, the transformer can treat the input like a set and still not lose the position information.</li>
</ol>
</section>
<section id="image-processing" class="level1">
<h1>Image Processing</h1>
<ol type="1">
<li>Image is a 2 dimensional grid(grayscale), the two dimensions are along height and width.</li>
<li>For an image of size say 100x100, we may split the image into 100 patches each of size 10x10 and convert the patches into embeddings(depth wise Convolution/linearize and dot product) and superimpose 2d positional information onto these embeddings and pass the sequence to the transformer.</li>
</ol>
</section>
<section id="graph-processing" class="level1">
<h1>Graph Processing</h1>
<ol type="1">
<li>In the text and image we saw that the funda was to destroy the structure of your input data and format it into a sequence of elements. Then identify the structural information that is essential for the task and add it to the representation of individual elements.</li>
<li>We can ignore the graph structure and create a sequence by randomly ordering nodes in the graph. This gives us the sequence we needed.</li>
<li>there is no notion of the ordering of nodes in a graph, so we can not add structural information to node embeddings based on the order.</li>
<li>Graph nodes do not have absolute positional information, any node can be the first node. they have relative positional information e.g.&nbsp;nodes at 1, 2, 3… hops away from a node.</li>
<li>When adding absolute positional information every element in the sequence has unique positional information added to it. But in relative positioning, the position of a node varies based on the node relative to which the measurement is taken. So if a graph has n nodes, each node has n relative position values 1 w.r.t each node in the graph.</li>
<li>Since each element in the sequence has multiple relative position values it is not possible to add them all to the representation of the element. <span class="citation" data-cites="shaw2018self">(<a href="#ref-shaw2018self" role="doc-biblioref">Shaw, Uszkoreit, and Vaswani 2018</a>)</span> suggests the following modifications to the attention mechanism to incorporate the relative position information. <span class="math display">\[\Large A_{ij} = \frac{x_{i}W^Q(x_jW^K+a_{ij}^K)^T}{\sqrt{d_z}}\]</span> <span class="math display">\[\Large z_i = \hat{A_{ij}}(x_jW^V+a_{ij}^V)\]</span></li>
<li><span class="math inline">\(\Large a_{ij}^K\)</span> and <span class="math inline">\(\Large a_{ij}^V\)</span> are learnt relative positional embeddings. key, value of components of <span class="math inline">\(\Large a_{ij}\)</span> are learnt seperately to simplify computation.</li>
<li>Recent work from <span class="citation" data-cites="park2022grpe">(<a href="#ref-park2022grpe" role="doc-biblioref">Park et al. 2022</a>)</span> shows that considering node-spatial, node-edge interactions when adding structural information improves the performance on drug property prediction tasks. They modify the attention mechanism as follows. <span class="math display">\[\Large b_{ij}^{spatial} = q_i\mathcal{P}_{\psi(i,j)}^{query}+k_j\mathcal{P}_{\psi(i,j)}^{key}\]</span> <span class="math display">\[\Large b_{ij}^{edge} = q_i\mathcal{E}_{e_{ij}}^{query}+k_j\mathcal{E}_{e_{ij}}^{key}\]</span> <span class="math display">\[\Large A_{ij} = \frac{q_i \cdot k_j + b_{ij}^{spatial} + b_{ij}^{edge}}{\sqrt{d_z}}\]</span> <span class="math display">\[\Large z_i = \sum_{j=1}^{N} \hat{A}_{ij}(v_j+\mathcal{P}_{\psi(i,j)}^{value}+\mathcal{E}_{e_{ij}}^{value})\]</span></li>
<li>Where <span class="math inline">\(\Large \psi(i,j)\)</span> is the shortest path distance between the nodes i, j. <span class="math inline">\(\Large e_{ij}\)</span> is the type of edge between node i, j(e.g: double bond, single bond etc). <span class="math inline">\(\Large \mathcal{P}\)</span> is the spatial encoding(distance), <span class="math inline">\(\Large \mathcal{E}\)</span> is the edge encoding(edge type). <span class="math inline">\(\Large b_{ij}^{spatial}\)</span> captures spatial relation between two nodes while considering the node-spatial interaction. <span class="math inline">\(\Large b_{ij}^{edge}\)</span> captures the edge between node i and j while considering node-edge interaction, the edge type was not considered by <span class="citation" data-cites="shaw2018self">(<a href="#ref-shaw2018self" role="doc-biblioref">Shaw, Uszkoreit, and Vaswani 2018</a>)</span>. Both <span class="math inline">\(\Large \mathcal{P}\)</span> and <span class="math inline">\(\Large \mathcal{E}\)</span> are learnt during training.</li>
<li>Attention modification visualized. <img src="assets/grpe.png" alt="drawing" width="800"></li>
<li><span class="citation" data-cites="park2022grpe">(<a href="#ref-park2022grpe" role="doc-biblioref">Park et al. 2022</a>)</span> improves performance on various datasets. <img src="assets/zinc.png" alt="1" width="800"> <img src="assets/molHIV.png" alt="2" width="800"> <img src="assets/molPCBA.png" alt="3" width="800"></li>
</ol>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-park2022grpe" class="csl-entry" role="doc-biblioentry">
Park, Wonpyo, Woong-Gi Chang, Donggeon Lee, Juntae Kim, et al. 2022. <span>“Grpe: Relative Positional Encoding for Graph Transformer.”</span> In <em>ICLR2022 Machine Learning for Drug Discovery</em>.
</div>
<div id="ref-shaw2018self" class="csl-entry" role="doc-biblioentry">
Shaw, Peter, Jakob Uszkoreit, and Ashish Vaswani. 2018. <span>“Self-Attention with Relative Position Representations.”</span> <em>arXiv Preprint arXiv:1803.02155</em>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>